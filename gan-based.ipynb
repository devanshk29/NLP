{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11301987,"sourceType":"datasetVersion","datasetId":7068070}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# GAN-based AI Text Detection\n# Complete implementation with PyTorch\n\nimport os\nimport re\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam, lr_scheduler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score\nfrom tqdm.notebook import tqdm\nimport spacy\nimport random\nfrom collections import Counter\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.manifold import TSNE\nfrom transformers import AutoTokenizer, AutoModel\nimport warnings\nimport joblib\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\n\n# Check for GPU availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Download required NLTK resources\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\n# Load spaCy model for NLP tasks\ntry:\n    nlp = spacy.load('en_core_web_sm')\nexcept OSError:\n    print(\"Downloading spaCy model...\")\n    spacy.cli.download('en_core_web_sm')\n    nlp = spacy.load('en_core_web_sm')\n\n# Configuration class for hyperparameters\nclass Config:\n    def __init__(self):\n        # Data processing\n        self.max_length = 512\n        self.masking_ratio = 0.15\n        self.min_text_length = 10\n        \n        # Model architecture\n        self.base_model_name = \"distilroberta-base\"\n        self.embedding_dim = 768\n        self.generator_hidden_dims = [512, 256, 128]\n        self.discriminator_hidden_dims = [128, 64, 32]\n        self.style_feature_dim = 32\n        self.dropout = 0.2\n        \n        # Training\n        self.batch_size = 16\n        self.num_epochs = 10\n        self.gen_lr = 2e-4\n        self.disc_lr = 1e-4\n        self.disc_steps = 5  # Train discriminator 5 times for each generator step\n        self.scheduler_patience = 2\n        self.scheduler_factor = 0.5\n        \n        # Paths\n        self.model_save_path = \"models/\"\n        self.best_model_path = os.path.join(self.model_save_path, \"best_gan_detector.pt\")\n        self.stylometric_features_path = os.path.join(self.model_save_path, \"stylometric_processor.joblib\")\n        \n        # Create directory if it doesn't exist\n        os.makedirs(self.model_save_path, exist_ok=True)\n\n# Initialize default config\nconfig = Config()\n\n# 1. Dataset Preprocessing\n\nclass TextPreprocessor:\n    def __init__(self, config):\n        self.config = config\n        self.tokenizer = AutoTokenizer.from_pretrained(config.base_model_name)\n        # Define content words POS tags to mask\n        self.content_pos = {'NOUN', 'ADJ', 'PROPN'}\n        # Store stylometric feature statistics\n        self.feature_stats = None\n    \n    def normalize_text(self, text):\n        \"\"\"Normalize text by converting to lowercase and standardizing characters\"\"\"\n        if not isinstance(text, str) or len(text) == 0:\n            return \"\"\n            \n        # Convert to lowercase and standardize whitespace\n        text = text.lower().strip()\n        # Standardize Unicode characters\n        text = re.sub(r'\\s+', ' ', text)\n        # Remove special characters while preserving punctuation and structure\n        text = re.sub(r'[^\\w\\s\\.,;:!?\\'\"-]', '', text)\n        return text\n    \n    def quality_filter(self, text):\n        \"\"\"Filter out extremely short texts or texts with abnormal character distributions\"\"\"\n        if not isinstance(text, str):\n            return False\n            \n        # Check minimum length\n        tokens = word_tokenize(text)\n        if len(tokens) < self.config.min_text_length:\n            return False\n            \n        # Check for abnormal character distribution\n        char_counts = Counter(text.lower())\n        alpha_ratio = sum(char_counts[c] for c in 'abcdefghijklmnopqrstuvwxyz') / len(text) if len(text) > 0 else 0\n        if alpha_ratio < 0.5:  # Less than 50% alphabetic characters\n            return False\n            \n        return True\n    \n    def extract_sentences(self, text):\n        \"\"\"Use spaCy to divide text into sentences\"\"\"\n        doc = nlp(text)\n        return [sent.text for sent in doc.sents]\n    \n    def strategic_masking(self, text):\n        \"\"\"Randomly mask content words while preserving function words\"\"\"\n        doc = nlp(text)\n        masked_tokens = []\n        \n        for token in doc:\n            # Skip punctuation\n            if token.is_punct:\n                masked_tokens.append(token.text)\n                continue\n                \n            # Determine if token should be masked\n            if token.pos_ in self.content_pos and random.random() < self.config.masking_ratio:\n                masked_tokens.append(\"[MASK]\")\n            else:\n                masked_tokens.append(token.text)\n                \n        return \" \".join(masked_tokens)\n    \n    def extract_stylometric_features(self, text):\n        \"\"\"Extract stylometric features from text\"\"\"\n        if not isinstance(text, str) or len(text) == 0:\n            return np.zeros(5)\n            \n        doc = nlp(text)\n        sentences = list(doc.sents)\n        \n        # Skip very short texts\n        if len(sentences) == 0:\n            return np.zeros(5)\n        \n        # Token and type counts\n        tokens = [token.text.lower() for token in doc if not token.is_punct]\n        types = set(tokens)\n        \n        # Type-token ratio\n        ttr = len(types) / len(tokens) if tokens else 0\n        \n        # Sentence length distribution\n        sent_lengths = [len(sent) for sent in sentences]\n        avg_sent_length = np.mean(sent_lengths) if sent_lengths else 0\n        std_sent_length = np.std(sent_lengths) if len(sent_lengths) > 1 else 0\n        \n        # POS tag distributions\n        pos_counts = Counter([token.pos_ for token in doc])\n        noun_ratio = pos_counts.get('NOUN', 0) / len(doc) if len(doc) > 0 else 0\n        func_word_ratio = sum(pos_counts.get(pos, 0) for pos in ['DET', 'ADP', 'CONJ', 'CCONJ', 'SCONJ']) / len(doc) if len(doc) > 0 else 0\n        \n        # Create feature vector\n        features = np.array([\n            ttr,\n            avg_sent_length,\n            std_sent_length,\n            noun_ratio,\n            func_word_ratio\n        ])\n        \n        return features\n    \n    def fit_transform_stylometric(self, texts):\n        \"\"\"Fit and transform stylometric features with normalization\"\"\"\n        features = []\n        for text in tqdm(texts, desc=\"Extracting stylometric features\"):\n            features.append(self.extract_stylometric_features(text))\n        \n        features_array = np.array(features)\n        \n        # Calculate mean and std for normalization\n        self.feature_stats = {\n            'mean': np.mean(features_array, axis=0),\n            'std': np.std(features_array, axis=0)\n        }\n        \n        # Avoid division by zero\n        self.feature_stats['std'] = np.where(self.feature_stats['std'] == 0, 1, self.feature_stats['std'])\n        \n        # Normalize\n        normalized_features = (features_array - self.feature_stats['mean']) / self.feature_stats['std']\n        \n        return normalized_features\n    \n    def transform_stylometric(self, texts):\n        \"\"\"Transform stylometric features using pre-fitted stats\"\"\"\n        if self.feature_stats is None:\n            raise ValueError(\"Stylometric feature processor has not been fitted yet\")\n        \n        features = []\n        for text in tqdm(texts, desc=\"Extracting stylometric features\"):\n            features.append(self.extract_stylometric_features(text))\n        \n        features_array = np.array(features)\n        \n        # Normalize using fitted stats\n        normalized_features = (features_array - self.feature_stats['mean']) / self.feature_stats['std']\n        \n        return normalized_features\n    \n    def preprocess_dataset(self, df, fit_stylometric=True):\n        \"\"\"Preprocess entire dataset\"\"\"\n        print(\"Preprocessing dataset...\")\n        \n        # Ensure text column exists\n        text_col = 'text' if 'text' in df.columns else df.columns[0]\n        label_col = 'label' if 'label' in df.columns else df.columns[1] if len(df.columns) > 1 else None\n        \n        # Normalize and filter texts\n        texts = []\n        labels = []\n        \n        for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Cleaning texts\"):\n            text = row[text_col]\n            label = row[label_col] if label_col else 0\n            \n            normalized_text = self.normalize_text(text)\n            if self.quality_filter(normalized_text):\n                texts.append(normalized_text)\n                labels.append(label)\n        \n        # Apply masking to texts\n        masked_texts = [self.strategic_masking(text) for text in tqdm(texts, desc=\"Applying masking\")]\n        \n        # Extract stylometric features\n        if fit_stylometric:\n            stylometric_features = self.fit_transform_stylometric(texts)\n        else:\n            stylometric_features = self.transform_stylometric(texts)\n        \n        # Save preprocessor for later use\n        joblib.dump(self, os.path.join(self.config.model_save_path, \"text_preprocessor.joblib\"))\n        \n        return texts, masked_texts, np.array(labels), stylometric_features\n\n# 2. Dataset Class\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, masked_texts, labels, stylometric_features, tokenizer, max_length):\n        self.texts = texts\n        self.masked_texts = masked_texts\n        self.labels = labels\n        self.stylometric_features = stylometric_features\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.masked_texts[idx]\n        label = self.labels[idx]\n        stylometric = self.stylometric_features[idx]\n        \n        # Tokenize text\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        # Get input_ids and attention_mask\n        input_ids = encoding['input_ids'].squeeze()\n        attention_mask = encoding['attention_mask'].squeeze()\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'stylometric': torch.tensor(stylometric, dtype=torch.float),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\ndef prepare_dataloaders(texts, masked_texts, labels, stylometric_features, config):\n    \"\"\"Split data and create dataloaders\"\"\"\n    # Split data into train, validation, and test sets\n    train_texts, temp_texts, train_masked, temp_masked, train_labels, temp_labels, train_style, temp_style = train_test_split(\n        texts, masked_texts, labels, stylometric_features, test_size=0.3, stratify=labels, random_state=SEED\n    )\n    \n    val_texts, test_texts, val_masked, test_masked, val_labels, test_labels, val_style, test_style = train_test_split(\n        temp_texts, temp_masked, temp_labels, temp_style, test_size=0.5, stratify=temp_labels, random_state=SEED\n    )\n    \n    # Create datasets\n    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name)\n    \n    train_dataset = TextDataset(\n        train_texts, train_masked, train_labels, train_style, tokenizer, config.max_length\n    )\n    \n    val_dataset = TextDataset(\n        val_texts, val_masked, val_labels, val_style, tokenizer, config.max_length\n    )\n    \n    test_dataset = TextDataset(\n        test_texts, test_masked, test_labels, test_style, tokenizer, config.max_length\n    )\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset, batch_size=config.batch_size, shuffle=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, batch_size=config.batch_size\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, batch_size=config.batch_size\n    )\n    \n    return train_loader, val_loader, test_loader\n\n# 3. Model Architecture\n\nclass BaseEmbeddingModel(nn.Module):\n    def __init__(self, model_name):\n        super(BaseEmbeddingModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        \n        # Freeze parameters to prevent fine-tuning\n        for param in self.model.parameters():\n            param.requires_grad = False\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        \n        # Get CLS token embedding (first token) and sequence embedding\n        cls_embedding = outputs.last_hidden_state[:, 0, :]\n        \n        return cls_embedding\n\nclass Generator(nn.Module):\n    def __init__(self, config):\n        super(Generator, self).__init__()\n        self.config = config\n        \n        # Create layers with residual connections\n        layers = []\n        input_dim = config.embedding_dim\n        \n        for i, dim in enumerate(config.generator_hidden_dims):\n            layers.append(nn.Linear(input_dim, dim))\n            layers.append(nn.LayerNorm(dim))\n            layers.append(nn.LeakyReLU(0.2))\n            layers.append(nn.Dropout(config.dropout))\n            input_dim = dim\n        \n        self.layers = nn.ModuleList(layers)\n        self.output_layer = nn.Linear(input_dim, config.generator_hidden_dims[-1])\n    \n    def forward(self, x):\n        residual = None\n        \n        for i in range(0, len(self.layers), 4):\n            if residual is not None and residual.size() == x.size():\n                x = x + residual\n            \n            # Store residual for next block\n            residual = x\n            \n            # Apply layer block\n            x = self.layers[i](x)      # Linear\n            x = self.layers[i+1](x)    # LayerNorm\n            x = self.layers[i+2](x)    # LeakyReLU\n            x = self.layers[i+3](x)    # Dropout\n        \n        # Final output\n        x = self.output_layer(x)\n        \n        return x\n\nclass StyleFeatureProcessor(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(StyleFeatureProcessor, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n\nclass SelfAttention(nn.Module):\n    def __init__(self, input_dim):\n        super(SelfAttention, self).__init__()\n        self.query = nn.Linear(input_dim, input_dim)\n        self.key = nn.Linear(input_dim, input_dim)\n        self.value = nn.Linear(input_dim, input_dim)\n        self.scale = torch.sqrt(torch.tensor(input_dim, dtype=torch.float32))\n    \n    def forward(self, x):\n        # x shape: (batch_size, input_dim)\n        x = x.unsqueeze(1)  # (batch_size, 1, input_dim)\n        \n        # Compute attention scores\n        q = self.query(x)  # (batch_size, 1, input_dim)\n        k = self.key(x)    # (batch_size, 1, input_dim)\n        v = self.value(x)  # (batch_size, 1, input_dim)\n        \n        # Attention weights\n        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale  # (batch_size, 1, 1)\n        attention = F.softmax(scores, dim=-1)\n        \n        # Apply attention to values\n        output = torch.matmul(attention, v)  # (batch_size, 1, input_dim)\n        output = output.squeeze(1)  # (batch_size, input_dim)\n        \n        return output\n\nclass Discriminator(nn.Module):\n    def __init__(self, config):\n        super(Discriminator, self).__init__()\n        self.config = config\n        \n        # Self-attention layer\n        self.attention = SelfAttention(config.generator_hidden_dims[-1])\n        \n        # Process stylometric features\n        self.style_processor = StyleFeatureProcessor(\n            5,  # Number of stylometric features\n            config.style_feature_dim,\n            config.style_feature_dim\n        )\n        \n        # Discriminator layers\n        input_dim = config.generator_hidden_dims[-1] + config.style_feature_dim\n        layers = []\n        \n        for hidden_dim in config.discriminator_hidden_dims:\n            layers.append(nn.Linear(input_dim, hidden_dim))\n            layers.append(nn.BatchNorm1d(hidden_dim))\n            layers.append(nn.LeakyReLU(0.2))\n            layers.append(nn.Dropout(config.dropout))\n            input_dim = hidden_dim\n        \n        self.layers = nn.ModuleList(layers)\n        \n        # Output layer\n        self.output_layer = nn.Linear(input_dim, 1)\n    \n    def forward(self, gen_features, stylometric_features):\n        # Apply attention to generator features\n        attended_features = self.attention(gen_features)\n        \n        # Process stylometric features\n        style_features = self.style_processor(stylometric_features)\n        \n        # Concatenate features\n        x = torch.cat([attended_features, style_features], dim=1)\n        \n        # Apply discriminator layers\n        for i in range(0, len(self.layers), 4):\n            x = self.layers[i](x)      # Linear\n            x = self.layers[i+1](x)    # BatchNorm\n            x = self.layers[i+2](x)    # LeakyReLU\n            x = self.layers[i+3](x)    # Dropout\n        \n        # Output layer\n        x = self.output_layer(x)\n        \n        return torch.sigmoid(x)\n\nclass GANTextDetector(nn.Module):\n    def __init__(self, config):\n        super(GANTextDetector, self).__init__()\n        self.config = config\n        \n        # Initialize components\n        self.embedding_model = BaseEmbeddingModel(config.base_model_name)\n        self.generator = Generator(config)\n        self.discriminator = Discriminator(config)\n    \n    def forward(self, input_ids, attention_mask, stylometric_features):\n        # Get embeddings\n        embeddings = self.embedding_model(input_ids, attention_mask)\n        \n        # Generate features\n        gen_features = self.generator(embeddings)\n        \n        # Discriminate\n        predictions = self.discriminator(gen_features, stylometric_features)\n        \n        return predictions, gen_features\n    \n    def get_embeddings(self, input_ids, attention_mask, stylometric_features):\n        \"\"\"Get embedding features for visualization or analysis\"\"\"\n        with torch.no_grad():\n            embeddings = self.embedding_model(input_ids, attention_mask)\n            gen_features = self.generator(embeddings)\n        \n        return gen_features\n\n# 4. Training Functions\n\ndef train_discriminator(model, batch, criterion, optimizer, device):\n    \"\"\"Train discriminator on one batch\"\"\"\n    # Get batch data\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    stylometric = batch['stylometric'].to(device)\n    labels = batch['label'].to(device).float().unsqueeze(1)\n    \n    # Zero gradients\n    optimizer.zero_grad()\n    \n    # Forward pass\n    predictions, _ = model(input_ids, attention_mask, stylometric)\n    \n    # Calculate loss\n    loss = criterion(predictions, labels)\n    \n    # Backward pass\n    loss.backward()\n    \n    # Update parameters\n    optimizer.step()\n    \n    return loss.item()\n\ndef train_generator(model, batch, criterion, optimizer, device):\n    \"\"\"Train generator on one batch\"\"\"\n    # Get batch data\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    stylometric = batch['stylometric'].to(device)\n    labels = batch['label'].to(device).float().unsqueeze(1)\n    \n    # Invert labels for adversarial training\n    inverted_labels = 1 - labels\n    \n    # Zero gradients\n    optimizer.zero_grad()\n    \n    # Forward pass (only train generator)\n    for param in model.discriminator.parameters():\n        param.requires_grad = False\n    \n    predictions, _ = model(input_ids, attention_mask, stylometric)\n    \n    # Calculate adversarial loss (fool the discriminator)\n    loss = criterion(predictions, inverted_labels)\n    \n    # Backward pass\n    loss.backward()\n    \n    # Update generator parameters\n    optimizer.step()\n    \n    # Re-enable discriminator gradients\n    for param in model.discriminator.parameters():\n        param.requires_grad = True\n    \n    return loss.item()\n\ndef validate(model, val_loader, criterion, device):\n    \"\"\"Validate model on validation set\"\"\"\n    model.eval()\n    val_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            # Get batch data\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            stylometric = batch['stylometric'].to(device)\n            labels = batch['label'].to(device).float().unsqueeze(1)\n            \n            # Forward pass\n            predictions, _ = model(input_ids, attention_mask, stylometric)\n            \n            # Calculate loss\n            loss = criterion(predictions, labels)\n            val_loss += loss.item()\n            \n            # Store predictions and labels for metrics\n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    val_loss /= len(val_loader)\n    all_preds = np.array(all_preds).flatten()\n    all_labels = np.array(all_labels).flatten()\n    \n    accuracy = accuracy_score(all_labels, (all_preds > 0.5).astype(int))\n    auc = roc_auc_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, (all_preds > 0.5).astype(int))\n    \n    return val_loss, accuracy, auc, f1\n\ndef train_gan_detector(model, train_loader, val_loader, config, device):\n    \"\"\"Train GAN text detector with alternating GAN training\"\"\"\n    # Initialize criterion and optimizers\n    criterion = nn.BCELoss()\n    discriminator_optimizer = Adam(\n        filter(lambda p: p.requires_grad, model.discriminator.parameters()),\n        lr=config.disc_lr\n    )\n    generator_optimizer = Adam(\n        model.generator.parameters(),\n        lr=config.gen_lr\n    )\n    \n    # Learning rate schedulers\n    disc_scheduler = lr_scheduler.ReduceLROnPlateau(\n        discriminator_optimizer, \n        mode='min', \n        factor=config.scheduler_factor,\n        patience=config.scheduler_patience\n    )\n    \n    gen_scheduler = lr_scheduler.ReduceLROnPlateau(\n        generator_optimizer, \n        mode='min', \n        factor=config.scheduler_factor,\n        patience=config.scheduler_patience\n    )\n    \n    # Training loop\n    best_val_auc = 0\n    train_losses = {'disc': [], 'gen': []}\n    val_metrics = {'loss': [], 'acc': [], 'auc': [], 'f1': []}\n    \n    for epoch in range(config.num_epochs):\n        model.train()\n        disc_loss_epoch = 0\n        gen_loss_epoch = 0\n        disc_steps = 0\n        gen_steps = 0\n        \n        # Track progress with tqdm\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n        \n        for i, batch in enumerate(progress_bar):\n            # Train discriminator\n            if i % (config.disc_steps + 1) != config.disc_steps:\n                disc_loss = train_discriminator(\n                    model, batch, criterion, discriminator_optimizer, device\n                )\n                disc_loss_epoch += disc_loss\n                disc_steps += 1\n                \n                # Update progress bar\n                progress_bar.set_postfix({\n                    'D_loss': f\"{disc_loss:.4f}\",\n                    'G_loss': f\"{gen_loss_epoch/max(1, gen_steps):.4f}\"\n                })\n            \n            # Train generator\n            else:\n                gen_loss = train_generator(\n                    model, batch, criterion, generator_optimizer, device\n                )\n                gen_loss_epoch += gen_loss\n                gen_steps += 1\n                \n                # Update progress bar\n                progress_bar.set_postfix({\n                    'D_loss': f\"{disc_loss_epoch/max(1, disc_steps):.4f}\",\n                    'G_loss': f\"{gen_loss:.4f}\"\n                })\n        \n        # Calculate average losses\n        avg_disc_loss = disc_loss_epoch / max(1, disc_steps)\n        avg_gen_loss = gen_loss_epoch / max(1, gen_steps)\n        \n        # Store training losses\n        train_losses['disc'].append(avg_disc_loss)\n        train_losses['gen'].append(avg_gen_loss)\n        \n        # Validate model\n        val_loss, val_acc, val_auc, val_f1 = validate(\n            model, val_loader, criterion, device\n        )\n        \n        # Store validation metrics\n        val_metrics['loss'].append(val_loss)\n        val_metrics['acc'].append(val_acc)\n        val_metrics['auc'].append(val_auc)\n        val_metrics['f1'].append(val_f1)\n        \n        # Update learning rate schedulers\n        disc_scheduler.step(val_loss)\n        gen_scheduler.step(val_loss)\n        \n        # Print epoch results\n        print(f\"Epoch {epoch+1}/{config.num_epochs}:\")\n        print(f\"  Train - Disc Loss: {avg_disc_loss:.4f}, Gen Loss: {avg_gen_loss:.4f}\")\n        print(f\"  Val - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}, F1: {val_f1:.4f}\")\n        \n        # Save best model\n        if val_auc > best_val_auc:\n            best_val_auc = val_auc\n            print(f\"  New best model with AUC: {val_auc:.4f}\")\n            \n            # Save model\n            torch.save({\n                'model_state_dict': model.state_dict(),\n                'config': config,\n                'epoch': epoch,\n                'val_auc': val_auc,\n                'val_acc': val_acc,\n                'val_f1': val_f1\n            }, config.best_model_path)\n    \n    # Plot training progress\n    plt.figure(figsize=(15, 5))\n    \n    # Plot training losses\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses['disc'], label='Discriminator')\n    plt.plot(train_losses['gen'], label='Generator')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training Losses')\n    plt.legend()\n    \n    # Plot validation metrics\n    plt.subplot(1, 2, 2)\n    plt.plot(val_metrics['loss'], label='Loss')\n    plt.plot(val_metrics['acc'], label='Accuracy')\n    plt.plot(val_metrics['auc'], label='AUC')\n    plt.plot(val_metrics['f1'], label='F1')\n    plt.xlabel('Epoch')\n    plt.ylabel('Score')\n    plt.title('Validation Metrics')\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(config.model_save_path, 'training_progress.png'))\n    plt.show()\n    \n    return model, train_losses, val_metrics\n\n# 5. Evaluation Functions\n\ndef evaluate_model(model, test_loader, device):\n    \"\"\"Evaluate model on test set\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_embeddings = []\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            # Get batch data\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            stylometric = batch['stylometric'].to(device)\n            labels = batch['label'].to(device).float().unsqueeze(1)\n            \n            # Forward pass\n            predictions, embeddings = model(input_ids, attention_mask, stylometric)\n            \n            # Store predictions, labels, and embeddings\n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_embeddings.extend(embeddings.cpu().numpy())\n    \n    # Convert to arrays\n    all_preds = np.array(all_preds).flatten()\n    all_labels = np.array(all_labels).flatten()\n    all_embeddings = np.array(all_embeddings)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, (all_preds > 0.5).astype(int))\n    auc = roc_auc_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, (all_preds > 0.5).astype(int))\n    \n    print(f\"Test Results:\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n    print(f\"  AUC-ROC: {auc:.4f}\")\n    print(f\"  F1 Score: {f1:.4f}\")\n    \n    # Visualize embeddings\n    visualize_embeddings(all_embeddings, all_labels)\n    \n    return accuracy, auc, f1, all_embeddings, all_preds, all_labels\n\ndef visualize_embeddings(embeddings, labels):\n    \"\"\"Visualize embeddings using t-SNE\"\"\"\n    # Reduce dimensionality with t-SNE\n    tsne = TSNE(n_components=2, random_state=SEED)\n    reduced_embeddings = tsne.fit_transform(embeddings)\n    \n    # Plot embeddings\n    plt.figure(figsize=(10, 8))\n    for label, color, marker, name in zip(\n        [0, 1], \n        ['blue', 'red'], \n        ['o', 'x'],\n        ['Human', 'AI']\n    ):\n        mask = labels == label\n        plt.scatter(\n            reduced_embeddings[mask, 0],\n            reduced_embeddings[mask, 1],\n            c=color,\n            marker=marker,\n            label=name,\n            alpha=0.7\n        )\n    \n    plt.legend()\n    plt.title('t-SNE Visualization of Text Embeddings')\n    plt.xlabel('t-SNE Dimension 1')\n    plt.ylabel('t-SNE Dimension 2')\n    plt.savefig('embedding_visualization.png')\n    plt.show()\n\n# 6. Single Text Prediction Function\n\ndef predict_single_text(text, model_path=None, device=None):\n    \"\"\"\n    Predict whether a single text is AI-generated or human-written\n    \n    Args:\n        text (str): The text to classify\n        model_path (str, optional): Path to the model. Defaults to None (uses default path).\n        device (torch.device, optional): Device to run inference on. Defaults to None.\n    \n    Returns:\n        float: Probability that the text is AI-generated\n        str: Classification (AI or Human)\n    \"\"\"\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load config and model\n    if model_path is None:\n        config = Config()\n        model_path = config.best_model_path\n    else:\n        config = Config()\n    \n    # Check if model exists\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model not found at {model_path}. Please train the model first.\")\n    \n    # Load preprocessor\n    preprocessor_path = os.path.join(config.model_save_path, \"text_preprocessor.joblib\")\n    if not os.path.exists(preprocessor_path):\n        raise FileNotFoundError(\"Text preprocessor not found. Please train the model first.\")\n    \n    preprocessor = joblib.load(preprocessor_path)\n    \n    # Load model\n    checkpoint = torch.load(model_path, map_location=device)\n    if 'config' in checkpoint:\n        config = checkpoint['config']\n    \n    # Initialize model\n    model = GANTextDetector(config)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n    \n    # Preprocess text\n    normalized_text = preprocessor.normalize_text(text)\n    masked_text = preprocessor.strategic_masking(normalized_text)\n    stylometric_features = preprocessor.transform_stylometric([normalized_text])[0]\n    \n    # Tokenize\n    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name)\n    encoding = tokenizer(\n        masked_text,\n        truncation=True,\n        max_length=config.max_length,\n        padding='max_length',\n        return_tensors='pt'\n    )\n    \n    # Move to device\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    stylometric = torch.tensor(stylometric_features, dtype=torch.float).unsqueeze(0).to(device)\n    \n    # Make prediction\n    with torch.no_grad():\n        prediction, _ = model(input_ids, attention_mask, stylometric)\n        probability = prediction.item()\n    \n    # Classify\n    classification = \"AI-generated\" if probability > 0.5 else \"Human-written\"\n    \n    return probability, classification\n\n# 7. Main Execution\n\ndef main(data_path, config):\n    \"\"\"Main execution function\"\"\"\n    # Load data\n    if data_path.endswith('.csv'):\n        df = pd.read_csv(data_path)\n    elif data_path.endswith('.tsv'):\n        df = pd.read_csv(data_path, sep='\\t')\n    else:\n        raise ValueError(\"Unsupported file format. Please provide a CSV or TSV file.\")\n    \n    # Initialize preprocessor\n    preprocessor = TextPreprocessor(config)\n    \n    # Preprocess data\n    texts, masked_texts, labels, stylometric_features = preprocessor.preprocess_dataset(df)\n    \n    # Prepare dataloaders\n    train_loader, val_loader, test_loader = prepare_dataloaders(\n        texts, masked_texts, labels, stylometric_features, config\n    )\n    \n    # Initialize model\n    model = GANTextDetector(config)\n    model.to(device)\n    \n    # Train model\n    model, train_losses, val_metrics = train_gan_detector(\n        model, train_loader, val_loader, config, device\n    )\n    \n    # Evaluate model\n    accuracy, auc, f1, embeddings, preds, labels = evaluate_model(\n        model, test_loader, device\n    )\n    \n    # Test single text prediction\n    sample_text = \"This is a sample text to demonstrate the prediction function.\"\n    probability, classification = predict_single_text(sample_text)\n    print(f\"\\nSample text prediction:\")\n    print(f\"  Text: \\\"{sample_text}\\\"\")\n    print(f\"  Probability of being AI-generated: {probability:.4f}\")\n    print(f\"  Classification: {classification}\")\n    \n    return model\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = Config()\n    \nconfig.batch_size = 128\nconfig.num_epochs = 50\nconfig.gen_lr = 2e-4\nconfig.disc_lr = 1e-4\n    \nmain('/kaggle/input/combined-dataset-ai-human/combined_data.csv', config)\n    \ntext = \"This is an example text that I want to classify as AI or human.\"\nprobability, classification = predict_single_text(text)\nprint(f\"The text is classified as {classification} with {probability:.2%} probability.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}